Ocorre que nem os cientistas da computação sabem como esses algoritmos funcionam: É por isso que eles os chamam de "caixas-pretas". O que se sabe é que os algoritmos, quando aplicados a um volume suficiente de dados de treinamento, passam a apresentar soluções para novos casos que parecem muito razoáveis.Como não compreendemos a "mecânica" de funcionamento do algoritmo, muitas vezes pode não ficar claro se o comportamento de tomada de decisão de uma aplicação de inteligência artificial é realmente "inteligente" ou se suas conclusões são apenas um sucesso mediano.Aferição da inteligência artificialFoi justamente essa pergunta - Quão inteligente é um algoritmo específico de inteligência artificial? - que se propôs responder uma equipe da Universidade de Tecnologia de Berlim, do Instituto Heinrich Hertz (Alemanha) e da Universidade de Tecnologia e Projetos de Cingapura.Para isso, eles criaram um programa de computador que permite automatizar a tarefa de aferição e quantificação dos resultados de um algoritmo de inteligência artificial.O método fundamental criado pela equipe, batizado de LRP (Layer-wise Relevance Propagation, propagação de relevância sensível a camadas, em tradução livre), permite visualizar quais dados de entrada sensibilizam uma dada decisão do sistema.Estendendo a LRP, a equipe desenvolveu uma técnica de análise de relevância espectral (SpRA: spectral relevance analysis), que é capaz de identificar e quantificar um amplo espectro de comportamentos de tomada de decisão aprendidos pelo programa de inteligência artificial.Desta forma, tornou-se agora possível detectar decisões indesejáveis, mesmo em conjuntos de dados muito grandes.Hans EspertoAo analisar sistemas de inteligência artificial já amplamente disseminados, dos usados em câmeras digitais para processamento de fotos até sistemas de diagnóstico médico por processamento de imagens, a equipe não se mostrou nada satisfeita com os resultados: A inteligência artificial não se mostrou tão inteligente quanto se esperava."Ficamos muito surpresos com a ampla gama de estratégias de resolução de problemas aprendidas [pelos algoritmos de IA]. Mesmo os sistemas modernos de inteligência artificial nem sempre encontram uma solução que pareça significativa do ponto de vista humano, mas às vezes usam a chamada 'Estratégia do Inteligente Hans'," conta o professor Wojciech Samek.O "Inteligente Hans", ou Hans Esperto, era um cavalo que supostamente sabia contar e foi considerado uma sensação científica durante os anos 1900. Como esclarecido mais tarde, o cavalo Hans não dominava a matemática, mas parecia ter bons "olhos psicológicos": Em 90% dos casos, ele conseguia obter a resposta correta pela linguagem corporal do seu treinador.Segundo a conclusão da equipe, os programas de inteligência artificial também poderiam ser catalogados na categoria de "espertos" no sentido atribuído ao cavalo Hans.Por exemplo, um sistema de inteligência artificial que ganhou várias competições internacionais de classificação de imagens há alguns anos usa uma estratégia que pode ser considerada ingênua do ponto de vista humano. Ele classifica as imagens principalmente com base no contexto: imagens foram atribuídas à categoria "navio" quando havia muita água na imagem, enquanto outras foram classificadas como "trem" quando apareciam trilhos. Várias outras imagens foram atribuídas à categoria correta por sua marca d'água de direitos autorais.A tarefa real - detectar os conceitos de navios ou trens - , portanto, não foi resolvida por esse sistema de IA, ainda que ele de fato classifique a maioria das imagens corretamente.Os pesquisadores também encontraram esse tipo de estratégia "defeituosa" de resolução de problemas em alguns dos algoritmos mais avançados de inteligência artificial, as chamadas redes neurais profundas - algoritmos que até agora eram considerados imunes a tais lapsos. As redes desse tipo analisadas baseiam sua decisão de classificação em parte em artefatos criados durante a preparação das imagens, não tendo nada a ver com o conteúdo real da imagem."É bastante concebível que cerca de metade dos sistemas de IA atualmente em uso, implícita ou explicitamente, se baseiem em estratégias do tipo 'Hans Esperto'. É hora de checar sistematicamente, para que sistemas seguros de IA possam ser desenvolvidos," disse o professor Klaus-Robert Muller."Nossa tecnologia automatizada é de código aberto e está disponível para todos os cientistas. Vemos nosso trabalho como um importante primeiro passo para tornar os sistemas de inteligência artificial mais robustos, explicáveis e seguros no futuro, e mais ainda será necessário. Este é um pré-requisito essencial para o uso geral da IA," concluiu Muller.Um único erro de um programa de inteligência artificial em um carro autônomo pode ser fatal. [Imagem: Cortesia MIT]In-inteligência realA professora Meredith Broussard, da Universidade de Nova York, não participou desta pesquisa, mas concorda com os alertas emitidos pela equipe."Quando as pessoas começam a pensar que a inteligência artificial é mais poderosa do que realmente é, elas começam a tomar decisões erradas," disse a pesquisadora, que é autora de um livro chamado Artificial Unintelligence, ("Ininteligência Artificial", em tradução livre)."A inteligência artificial não é realmente inteligente. IA é apenas matemática," acrescenta ela, destacando que a matemática não consegue lidar com todos os tipos de problemas.A pesquisadora mostra-se preocupada sobretudo com o uso de programas de inteligência artificial nos carros sem motorista. Vários testes ao redor do mundo têm mostrado que é fácil ludibriar os programas desses veículos, com características geométricas típicas nas imagens gerando falsos positivos ou falsos negativos - de sinais de pare ou de pedestres, por exemplo.